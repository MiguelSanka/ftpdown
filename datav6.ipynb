{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miguel.sanchez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\miguel.sanchez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paramiko\\transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "#from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "#import chardet\n",
    "#import csv\n",
    "\n",
    "#load_dotenv()\n",
    "hostname = \"auditsa.com.mx\"\n",
    "username = \"multimedios\"\n",
    "password = \"M260L2t41M3d10$\"\n",
    "port = 22\n",
    "local = \"C:/Users/miguel.sanchez/Desktop/FTP-data-downloader-main/\"\n",
    "\n",
    "\n",
    "# Suerclasbe para mejorar la velocidad de descarga\n",
    "class FastTransport(paramiko.Transport):\n",
    "    def __init__(self, sock):\n",
    "        super(FastTransport, self).__init__(sock)\n",
    "        self.window_size = paramiko.common.MAX_WINDOW_SIZE\n",
    "        self.packetizer.REKEY_BYTES = pow(2, 40)\n",
    "        self.packetizer.REKEY_PACKETS = pow(2, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_today_files() -> list:\n",
    "    last_day = datetime.today().date() \n",
    "    ssh_conn = FastTransport((hostname, port))\n",
    "    ssh_conn.connect(username=username, password=password)\n",
    "    sftp = paramiko.SFTPClient.from_transport(ssh_conn)\n",
    "\n",
    "    files = sftp.listdir_attr()\n",
    "    files_with_dates = [(file.filename, datetime.fromtimestamp(file.st_mtime).date()) for file in files]\n",
    "    today = [file for file in files_with_dates if file[1] == last_day]\n",
    "\n",
    "    remote_files = []\n",
    "    for remote_file in today:\n",
    "        #if remote_file[0].endswith(\".rar\"):\n",
    "        remote_files.append(remote_file[0]) \n",
    "\n",
    "    sftp.close()\n",
    "    ssh_conn.close()\n",
    "\n",
    "    return remote_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = find_today_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multimedios_1674_Radio_20240826_20240901.txt',\n",
       " 'Multimedios_1674_Tv_20240826_20240901.txt',\n",
       " 'Multimedios_1993_Radio_20240826_20240901.txt',\n",
       " 'Multimedios_1993_Tv_20240826_20240901.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_r):\n",
    "    try:\n",
    "        ssh_conn = FastTransport((hostname, port))\n",
    "        ssh_conn.connect(username=username, password=password)\n",
    "        sftp = paramiko.SFTPClient.from_transport(ssh_conn)\n",
    "\n",
    "        media = \"tv\" if \"Tv\" in file_r else \"radio\"\n",
    "        kind = \"testigos\" if \"1674\" in file_r else \"comercial\"\n",
    "\n",
    "        if file_r.endswith(\".txt\"):\n",
    "            #Intenta abrir el archivo de manera remota y hace un df con este\n",
    "            try: \n",
    "                with sftp.open(file_r, \"r\") as ftp_file:\n",
    "                    df = pd.read_csv(ftp_file, sep=\"|\", low_memory=False, encoding_errors=\"replace\")\n",
    "            #Si falla, intenta descargarlo\n",
    "            except:\n",
    "                sftp.get(file_r, local + file_r)\n",
    "                df = pd.read_csv(local + file_r, sep=\"|\", encoding=\"utf-8-sig\", encoding_errors=\"replace\", low_memory=False)\n",
    "\n",
    "            df[\"HIT_Fecha\"] = pd.to_datetime(df[\"HIT_Fecha\"], dayfirst=False)\n",
    "            last_day = df[\"HIT_Fecha\"].max() \n",
    "\n",
    "            # Divide el archivo en 7 días\n",
    "            for i in range(0, 7):\n",
    "                d = (last_day - timedelta(days=i)).date()\n",
    "                filtered = df.query(\"HIT_Fecha == @d\")\n",
    "                filtered.to_csv(f\"{local}{str(d)}_{media}_auditsa_{kind}.csv\", index=False, mode='w', header=True, sep=\"|\", encoding=\"utf-8-sig\")\n",
    "\n",
    "        if file_r.endswith(\".rar\"):\n",
    "            sftp.get(file_r, local + file_r)\n",
    "            ##time.sleep(220) ## Wait 200 seconds until the .rar file is download, then extract it.\n",
    "            import rarfile\n",
    "            rarfile.UNRAR_TOOL = 'C://Program Files//WinRAR//UNrar.exe' \n",
    "            with rarfile.RarFile(local + file_r) as rf:\n",
    "                # Extraer todos los archivos en el directorio de destino\n",
    "                rf.extractall(path=local)\n",
    "                print(f'Archivo extraído en {local}')\n",
    "\n",
    "            list_of_files = glob.glob(f'{local}*.txt') # gets lastest .txt\n",
    "            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "            print(latest_file)\n",
    "                \n",
    "            data = pd.read_csv(latest_file, sep=\"|\")\n",
    "            data[\"HIT_Fecha\"] = pd.to_datetime(data[\"HIT_Fecha\"], dayfirst=False)\n",
    "                \n",
    "            try: \n",
    "                for n in range(1, 32):\n",
    "                    filtered = data[data[\"HIT_Fecha\"].dt.day == n]\n",
    "                    if filtered.empty == False:\n",
    "                        date = filtered[\"HIT_Fecha\"].iloc[0].strftime('%Y-%m-%d')\n",
    "                        filtered.to_csv(f\"{local}{date}_{media}_auditsa_{kind}.csv\", index = False, sep=\"|\", encoding=\"utf-8-sig\")\n",
    "\n",
    "                print(\"csv files created from \" + local + file_r)\n",
    "            except:\n",
    "                print(\"Error when creating csv files!\")\n",
    "\n",
    "        if os.path.isfile(local + file_r):\n",
    "            os.remove(local + file_r)\n",
    "            print(\"Deleted \" + local + file_r)\n",
    "\n",
    "        sftp.close()\n",
    "        ssh_conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        print(f\"Exception type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar la funcion en paralelo con los 4 archivos que se subieron hoy\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(download_file, day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
